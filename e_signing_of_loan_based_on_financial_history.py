# -*- coding: utf-8 -*-
"""Project 5 - E-Signing of Loan Based on Financial History.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FpUa2AlkljdOgcLfVI5NQrn8k53D7cPv

# Part 1: Data preprocessing

Dataset link: https://www.kaggle.com/aniruddhachoudhury/esigning-of-loan-based-on-financial-history

## Importing the dataset and libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

dataset = pd.read_csv('/content/financial_data.csv')

"""## Data exploration"""

dataset.head()

dataset.shape

# check the columns
dataset.columns

# check the information about the dataset
dataset.info()

# statistical summary
dataset.describe()

"""## Dealing with missing data"""

# check if there are any null values
dataset.isnull().values.any()

# check how many null values
dataset.isnull().values.sum()

# show the null values using the heatmap
plt.figure(figsize=(16,9))
sns.heatmap(data=dataset.isnull(), cmap='coolwarm')
plt.show()

"""## Encoding the categorical data"""

dataset.head()

# check columns with categorical values
dataset.select_dtypes(include='object').columns

len(dataset.select_dtypes(include='object').columns)

# unique values in categorical column
dataset['pay_schedule'].unique()

dataset['pay_schedule'].nunique()

dataset.head()

dataset.shape

# encode categorical variable
dataset = pd.get_dummies(data=dataset, drop_first=True)

dataset.head()

dataset.shape

"""## Countplot"""

sns.countplot(dataset['e_signed'], label='Count')
plt.show()

# e-signed values
(dataset.e_signed == 1).sum()

# not e-signed values
(dataset.e_signed == 0).sum()

"""## Correlation matrix and Heatmap"""

dataset.head()

dataset_2 = dataset.drop(columns=['entry_id', 'e_signed'])

dataset_2.shape

dataset_2.corrwith(dataset.e_signed).plot.bar(
    figsize=(16,9), title = 'Correlation with E Signed',
    rot = 45, grid = True)

# dataset.e_signed: responce variable
# rot: rotation

# Create Correlation Matrix
corr = dataset.corr()

corr

# correlation heatmap
plt.figure(figsize=(16,9))
ax = sns.heatmap(corr, annot=True, linewidths=2)

corr_2 = corr[corr > 0.2]

# correlation heatmap
plt.figure(figsize=(16,9))
ax = sns.heatmap(corr_2, annot=True, linewidths=2)

"""## Restructure the dataset"""

dataset.head()

# combine two columns 'personal_account_m' and 'personal_account_y'
dataset['employment months'] = (dataset.months_employed + (dataset.years_employed*12))

dataset.head()

# drop the columns
dataset = dataset.drop(columns=['months_employed', 'years_employed'])

dataset.head()

# combine two columns 'personal_account_m' and 'personal_account_y'
dataset['personal_account_months'] = (dataset.personal_account_m + (dataset.personal_account_y*12))

dataset.head()

dataset = dataset.drop(columns=['personal_account_m', 'personal_account_y'])

dataset.head()

dataset.shape

dataset.columns

"""## Split the dataset into train and test set"""

dataset.head()

# matrix of features
x = dataset.drop(columns=['entry_id', 'e_signed'])

x.shape

# target/dependent variable
y = dataset['e_signed']

y.shape

# split the dataset
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

x_train.shape

x_test.shape

y_train.shape

y_test.shape

"""## Feature scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

x_train

x_test

"""# Part 2: Building the model

## 1) Logistic regression
"""

from sklearn.linear_model import LogisticRegression
classifier_lr = LogisticRegression(random_state = 0)
classifier_lr.fit(x_train, y_train)

y_pred = classifier_lr.predict(x_test)

from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score

acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

results = pd.DataFrame([['Logistic Regression', acc, prec, rec, f1]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])

results

"""## 2) SVM (linear)"""

from sklearn.svm import SVC
classifier_svm = SVC(kernel = 'linear', random_state = 0)
classifier_svm.fit(x_train, y_train)

y_pred = classifier_svm.predict(x_test)

acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

model_results = pd.DataFrame([['SVM_linear', acc, prec, rec, f1]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])

results = results.append(model_results, ignore_index = True)

results

"""## 3) SVM (rbf)"""

from sklearn.svm import SVC
classifier_svm_2 = SVC(kernel = 'rbf', random_state = 0)
classifier_svm_2.fit(x_train, y_train)

y_pred = classifier_svm_2.predict(x_test)

acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

model_results = pd.DataFrame([['SVM_rbf', acc, prec, rec, f1]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])


results = results.append(model_results, ignore_index = True)
results

"""## 4) Random forest"""

from sklearn.ensemble import RandomForestClassifier
classifier_rf = RandomForestClassifier(random_state = 0, n_estimators = 100,
                                    criterion = 'entropy')
classifier_rf.fit(x_train, y_train)

y_pred = classifier_rf.predict(x_test)

acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

model_results = pd.DataFrame([['Random Forest', acc, prec, rec, f1]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])


results = results.append(model_results, ignore_index = True)
results

"""### K-fold Cross validation and confusion matrix"""

# k-Fold Cross Validation technique will create 10 training test folds
# that means we will get 10 accuracies and will compute the average of these 10 accuracies

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator=classifier_rf, X=x_train, y=y_train, cv=10)
print("Accuracy is {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation is {:.2f} %".format(accuracies.std()*100))

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

"""## 5) XGBoost"""

from xgboost import XGBClassifier
classifier_xgb = XGBClassifier(random_state=0)
classifier_xgb.fit(x_train, y_train)

y_pred = classifier_xgb.predict(x_test)

acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

model_results = pd.DataFrame([['XGBoost', acc, prec, rec, f1]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])


results = results.append(model_results, ignore_index = True)
results

"""### K-fold Cross validation and confusion matrix"""

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator=classifier_xgb, X=x_train, y=y_train, cv=10)

print("Accuracy is {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation is {:.2f} %".format(accuracies.std()*100))

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
cm = confusion_matrix(y_test, y_pred)
print(cm)

"""# Part 3: Applying Randomized Search to find the best parameters

1. Parameters in XGboost: https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters
"""

from sklearn.model_selection import RandomizedSearchCV

parameters ={
    'learning_rate': [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],
    'max_depth':[3, 4, 5, 6, 8, 10, 12, 15],
    'min_child_weight':[1, 3, 5, 7],
    'gamma':[0.0, 0.1, 0.2 , 0.3, 0.4],
    'colsample_bytree':[0.3, 0.4, 0.5 , 0.7]
    }

randomized_search = RandomizedSearchCV(estimator = classifier_xgb, param_distributions = parameters,
                                 n_iter = 5, scoring='roc_auc', n_jobs = -1, cv = 5, verbose=3)

# cv: cross-validation
# n_jobs = -1:
# Number of jobs to run in parallel. -1 means using all processors

randomized_search.fit(x_train, y_train)

randomized_search.best_estimator_
# all the params that are selected by the randomized_search for that XGB Classifier

randomized_search.best_params_

randomized_search.best_score_

asdf

"""# Part 4: Final Model (XGBoost)"""

from xgboost import XGBClassifier
classifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0.2,
              learning_rate=0.05, max_delta_step=0, max_depth=10,
              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,
              nthread=None, objective='binary:logistic', random_state=0,
              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
              silent=None, subsample=1, verbosity=1)

classifier.fit(x_train, y_train)

y_pred = classifier.predict(x_test)

acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

final_results = pd.DataFrame([['XGBoost', acc, prec, rec, f1]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])


final_results

"""## k-fold cross validation and confusion matrix"""

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator=classifier, X=x_train, y=y_train, cv=10)

print("Accuracy is {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation is {:.2f} %".format(accuracies.std()*100))

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
cm = confusion_matrix(y_test, y_pred)
print(cm)

"""## Part 6: Predicting a single observation"""

single_obs = [[619,	42,	2,	0.00,	1,	1,	1,	101348.88, 0,	0, 0]]

print(classifier.predict(sc.transform(single_obs)))

